# 环境
环境状态函数的转移方程
$$
\text{下一状态} = f(\text{当前状态}, \text{智能体的动作})
$$
# 目标
智能体和环境每次进行交互时, 环境会产生相应的奖励信号. 整个交互过程的每一轮获得的奖励信号可以进行累加, 形成智能体的整体回报（return）, 好比一盘游戏最后的分数值. 强化学习的优化目标就是这个回报的期望（定义为价值, value）

# 数据
有监督学习的任务建立在从给定的数据分布中采样得到的训练数据集上, 通过优化在训练数据集中设定的目标函数（如最小化预测误差）来找到模型的最优参数. 这里, 训练数据集背后的数据分布是完全不变的. 

在强化学习中, 数据是在智能体与环境交互的过程中得到的. 如果智能体不采取某个决策动作, 那么该动作对应的数据就永远无法被观测到, 所以当前智能体的训练数据来自之前智能体的决策结果. 因此, 智能体的策略不同, 与环境交互所产生的数据分布就不同.

具体参见占用度量（[[Occupancy Measure]]）

# 独特性
监督学习寻找一个最小化损失函数的最优模型
强化学习寻找一种最大化奖励函数的最优策略





