In [[Denoising Diffusion Probabilistic Models (DDPM)|DDPM]], we focus on modeling just the data distribution $p(\boldsymbol x)$. However, we are often also interested in learning conditional distribution $p(\boldsymbol x | y)$, which would enable us to explicitly control the data we generate through **conditional information** $y$. 

A natural way to add conditioning information is simply alongside the timestep information, at each iteration. Recall that from the joint distribution of $\boldsymbol x_1, \ldots ,\boldsymbol x_T$ can be derived from the product of [[Denoising Diffusion Probabilistic Models (DDPM)#Transition Distribution|transition distributions]]
$$
p(\boldsymbol x_{0:T}) = p(\boldsymbol x_T) \prod_{t = 1}^T p_{\boldsymbol \theta}(\boldsymbol x_{t - 1} | \boldsymbol x_t)
$$
We can simply add arbitrary conditioning information $y$ at each transition step as
$$
p(\boldsymbol x_{0:T}| y) = p(\boldsymbol x_T) \prod_{t = 1}^T p_{\boldsymbol \theta}(\boldsymbol x_{t - 1} | \boldsymbol x_t | y)
$$
where $y$ could be a text encoding in image-text generation, or a low-resolution image to perform super-resolution on. Now we can learn the core neural network of a VDM as before.

However, a caveat of this vanilla formulation is that **a conditional diffusion model trained in this way may potentially learn to ignore or downplay any given conditioning information**. Guidance is therefore proposed as a way to more explicitly control the amount of weight the model gives to the conditioning information, at the cost of sample diversity

# Classifier Guidance
Strat with the [[Denoising Diffusion Probabilistic Models (DDPM)#Score-Matching Langevin Dynamics (SMLD)|score-based formulation]] of a diffusion model, where our goal is to learn $\nabla \log p(\boldsymbol x_t | y)$. By [[Bayes' Theorem|Bayes' rule]], we can derive
$$
\begin{aligned}
\nabla \log p(\boldsymbol x_t | y) &= \nabla \log \left( \dfrac{p(\boldsymbol x_t)p(y| \boldsymbol x_t)}{p(y)} \right)  \\
&= \nabla \log p(\boldsymbol x_t) + \nabla \log p(y | \boldsymbol x_t) - \nabla \log p(y) \\
&= \underbrace{\nabla \log p(\boldsymbol x_t)}_{\text{unconditional score}} + \underbrace{\nabla\log p(y | \boldsymbol x_t)}_{\text{adversarial gradient}}
\end{aligned}
$$
Therefore, in Classifier Guidance, the score of an unconditional diffusion model is learned as previously derived, alongside a classifier that takes in arbitrary noisy $\boldsymbol x_t$ and attempts to predict conditional information $y$. Then, during the sampling procedure, the overall conditional score function used for annealed [[Denoising Diffusion Probabilistic Models (DDPM)#Langevin Dynamics|Langevin Dynamics]] is computed as the sum of the unconditional score function and the adversarial gradient of the noisy classifier

To introduce fine-grained control to either encourage or discourage the model to consider the conditioning information, we can scales the adversarial gradient of the noisy classifier by a $\gamma$ hyper-parameter
$$
\nabla \log p(\boldsymbol x_t | y) = \nabla \log p (\boldsymbol x_t) + \gamma \nabla p (y | \boldsymbol x_t)
$$
The higher $\gamma$ is, the model learns to produce samples that heavier adhere to the conditioning information, which comes at the cost of sample diversity.

Here is a pseudo-code of classifier guidance:
``` python
classifier_model = ...  # Load a pre-trained image classification model  
y = 1  # We want to generate an image of class 1, let's assume class 1 corresponds to the "cat" category  
guidance_scale = 7.5  # Controls the strength of the class guidance, the higher the stronger  
input = get_noise(...)  # Randomly draw noise with the same shape as the output image from a Gaussian distribution  
  
for t in tqdm(scheduler.timesteps):  
  
    # Use neural networks for inference, to predict noise (that is, the first term in the euqation above)  
    with torch.no_grad():  
        noise_pred = model(input, t).sample  
  
    # Classifier guidance step  
    class_guidance = classifier_model.get_class_guidance(input, y) # Compute gradient using classifier (second term in equation)  
    noise_pred += class_guidance * guidance_scale  # Apply the gradient   
  
    # Calculate x_{t-1} using the updated noise  
    input = scheduler.step(noise_pred, t, input).prev_sample
```
In this implementation, the class guidance is generated by a pre-trained classifier
# Classifier-Free Guidance
Classifier Guidance can **only control the categories generated by the classification model.** If the classification model distinguishes 10 classes, then Classifier Guidance can only guide the diffusion model to generate those fixed 10 classes.

Therefore, Classifier-Free Guidance is introduced to solve the issue. By rearranging the first equation in Classifier Guidance
$$
\nabla \log p(y | \boldsymbol x_t) = \nabla \log p(\boldsymbol x_t | y) - \nabla \log p(\boldsymbol x_t)
$$
And substituting this into the second equation
$$
\begin{aligned}
\nabla \log p(\boldsymbol x_t | y) &= \nabla \log p(\boldsymbol x_t) + \gamma \left( \nabla \log p(\boldsymbol x_t | y) - \nabla \log p(\boldsymbol x_t)\right)  \\
&= \gamma \nabla \log p(\boldsymbol x_t | y) + (1-\gamma) \nabla \log p(\boldsymbol x_t)
\end{aligned}
$$
Once again, $\gamma$ is a term that controls how much our learned conditional model cares about the conditioning information.

Another example of where the condition is text
``` python
clip_model = ... # Load an official CLIP model  
  
text = "a dog" # Input text  
text_embeddings = clip_model.text_encode(text) # Encode the conditional text  
empty_embeddings = clip_model.text_encode("") # Encode empty text  
text_embeddings = torch.cat([empty_embeddings, text_embeddings]) # Concatenate them together as the condition  
  
input = get_noise(...) # Randomly draw noise with the same shape as the output image from a Gaussian distribution  
  
for t in tqdm(scheduler.timesteps):  
  
# Use UNet for inference, to predict noise  
with torch.no_grad():  
# Here we predict noise for both images with text and images with empty text  
noise_pred = model(input, t, encoder_hidden_states=text_embeddings).sample  
  
# Classifier-Free Guidance guidance  
noise_pred_uncond, noise_pred_text = noise_pred.chunk(2) # Split into unconditional and conditional noise  
# Consider the vector from "unconditional noise" towards "conditional noise",  
# and scale this vector according to the value of guidance_scale  
noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)  
  
# Calculate x_t-1 using the predicted noise_pred and x_t  
input = scheduler.step(noise_pred, t, input).prev_sample
```

In the [[U-Net|U-Net]] architecture, for examples, the text embeddings are typically concatenated with the input features at certain stages (e.g., the bottleneck or skip connections). This allows the U-Net to leverage the contextual information provided by the text when making predictions.



